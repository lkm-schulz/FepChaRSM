Files  local:///opt/sparkbench/sparkbench_2.12-1.0.jar from /opt/sparkbench/sparkbench_2.12-1.0.jar to /opt/spark/work-dir/./sparkbench_2.12-1.0.jar
24/06/12 17:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/12 17:18:45 INFO HiveConf: Found configuration file null
24/06/12 17:18:45 INFO SparkContext: Running Spark version 3.4.1
24/06/12 17:18:46 INFO ResourceUtils: ==============================================================
24/06/12 17:18:46 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/12 17:18:46 INFO ResourceUtils: ==============================================================
24/06/12 17:18:46 INFO SparkContext: Submitted application: Data Generator
24/06/12 17:18:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 24576, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/12 17:18:46 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
24/06/12 17:18:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/12 17:18:46 INFO SecurityManager: Changing view acls to: spark,lennart
24/06/12 17:18:46 INFO SecurityManager: Changing modify acls to: spark,lennart
24/06/12 17:18:46 INFO SecurityManager: Changing view acls groups to: 
24/06/12 17:18:46 INFO SecurityManager: Changing modify acls groups to: 
24/06/12 17:18:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark, lennart; groups with view permissions: EMPTY; users with modify permissions: spark, lennart; groups with modify permissions: EMPTY
24/06/12 17:18:46 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
24/06/12 17:18:46 INFO SparkEnv: Registering MapOutputTracker
24/06/12 17:18:46 INFO SparkEnv: Registering BlockManagerMaster
24/06/12 17:18:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/12 17:18:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/12 17:18:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/12 17:18:46 INFO DiskBlockManager: Created local directory at /var/data/spark-d96b2f39-a5fb-48e7-a6ee-3df1e3ef68e9/blockmgr-6a2e8fe5-2697-45f1-8da3-02c800f7cbdb
24/06/12 17:18:46 INFO MemoryStore: MemoryStore started with capacity 11.8 GiB
24/06/12 17:18:46 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/12 17:18:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/12 17:18:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/12 17:18:47 INFO SparkContext: Added JAR local:/opt/sparkbench/sparkbench_2.12-1.0.jar at file:/opt/sparkbench/sparkbench_2.12-1.0.jar with timestamp 1718212725975
24/06/12 17:18:47 WARN SparkContext: The JAR local:///opt/sparkbench/sparkbench_2.12-1.0.jar at file:/opt/sparkbench/sparkbench_2.12-1.0.jar has been added already. Overwriting of added jar is not supported in the current version.
24/06/12 17:18:47 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
24/06/12 17:18:49 INFO ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 5, known: 0, sharedSlotFromPendingPods: 2147483647.
24/06/12 17:18:49 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/06/12 17:18:49 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/06/12 17:18:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
24/06/12 17:18:49 INFO NettyBlockTransferService: Server created on sparkbench-090470900d7562a7-driver-svc.default.svc 10.244.2.48:7079
24/06/12 17:18:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/12 17:18:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, sparkbench-090470900d7562a7-driver-svc.default.svc, 7079, None)
24/06/12 17:18:49 INFO BlockManagerMasterEndpoint: Registering block manager sparkbench-090470900d7562a7-driver-svc.default.svc:7079 with 11.8 GiB RAM, BlockManagerId(driver, sparkbench-090470900d7562a7-driver-svc.default.svc, 7079, None)
24/06/12 17:18:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, sparkbench-090470900d7562a7-driver-svc.default.svc, 7079, None)
24/06/12 17:18:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, sparkbench-090470900d7562a7-driver-svc.default.svc, 7079, None)
24/06/12 17:18:49 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/06/12 17:18:49 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/06/12 17:18:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
24/06/12 17:18:50 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
24/06/12 17:18:50 INFO MetricsSystemImpl: s3a-file-system metrics system started
24/06/12 17:18:50 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 5, known: 3, sharedSlotFromPendingPods: 2147483644.
24/06/12 17:18:50 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/06/12 17:18:50 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/06/12 17:18:51 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
24/06/12 17:18:51 INFO SingleEventLogFileWriter: Logging events to s3a://logs/spark-events/spark-864862eb5a6b4d8c91a3448b54d6a3b3.inprogress
24/06/12 17:18:51 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-events/spark-864862eb5a6b4d8c91a3448b54d6a3b3.inprogress. This is unsupported
24/06/12 17:18:56 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.8.58:40742
24/06/12 17:18:56 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.3.55:34630
24/06/12 17:18:56 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.10.64:54792
24/06/12 17:18:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.10.64:54802) with ID 2,  ResourceProfileId 0
24/06/12 17:18:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.8.58:40744) with ID 1,  ResourceProfileId 0
24/06/12 17:18:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.3.55:34642) with ID 3,  ResourceProfileId 0
24/06/12 17:18:57 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.10.64:34279 with 14.2 GiB RAM, BlockManagerId(2, 10.244.10.64, 34279, None)
24/06/12 17:18:57 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.8.58:39339 with 14.2 GiB RAM, BlockManagerId(1, 10.244.8.58, 39339, None)
24/06/12 17:18:57 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.3.55:38453 with 14.2 GiB RAM, BlockManagerId(3, 10.244.3.55, 38453, None)
24/06/12 17:18:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.1.60:41098
24/06/12 17:18:58 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.11.65:55670
24/06/12 17:18:58 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.1.60:41114) with ID 4,  ResourceProfileId 0
24/06/12 17:18:58 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
24/06/12 17:18:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.1.60:39291 with 14.2 GiB RAM, BlockManagerId(4, 10.244.1.60, 39291, None)
Custom start time given: 1718212757
24/06/12 17:18:58 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/12 17:18:58 INFO SparkUI: Stopped Spark web UI at http://sparkbench-090470900d7562a7-driver-svc.default.svc:4040
24/06/12 17:18:58 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
24/06/12 17:18:58 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
24/06/12 17:18:58 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.
24/06/12 17:18:58 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Unknown Source)
24/06/12 17:18:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/12 17:18:59 INFO MemoryStore: MemoryStore cleared
24/06/12 17:18:59 INFO BlockManager: BlockManager stopped
24/06/12 17:18:59 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/12 17:18:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/12 17:18:59 INFO SparkContext: Successfully stopped SparkContext
Exception in thread "main" java.io.FileNotFoundException: /opt/sparkbench/workloads/bursty-spaced-i10d60s600_2.csv (No such file or directory)
	at java.base/java.io.FileInputStream.open0(Native Method)
	at java.base/java.io.FileInputStream.open(Unknown Source)
	at java.base/java.io.FileInputStream.<init>(Unknown Source)
	at scala.io.Source$.fromFile(Source.scala:94)
	at scala.io.Source$.fromFile(Source.scala:79)
	at scala.io.Source$.fromFile(Source.scala:57)
	at Workload$.fromFile(Workload.scala:48)
	at Sparkbench$.main(Sparkbench.scala:40)
	at Sparkbench.main(Sparkbench.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
24/06/12 17:18:59 INFO ShutdownHookManager: Shutdown hook called
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd1da361-c3ac-43b2-865b-31b215fca3d4
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdad261c-a41a-4eb7-bab2-ca450a711de5
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-cfb1eeed-8801-4c89-8427-d5dd08d979cc
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /var/data/spark-d96b2f39-a5fb-48e7-a6ee-3df1e3ef68e9/spark-21ff2feb-a10f-4aaa-855b-a700b5fe4a34
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2cbcc79-00fd-40cc-b90d-28a2d90e8de6
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-24385b0f-255c-4330-a771-07d725315b0c
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a732e65-6a1a-4872-969a-bcc316022e4e
24/06/12 17:18:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-c40fe4dc-ede4-401e-bfc8-bfc1cdeaaeb5
24/06/12 17:18:59 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
24/06/12 17:18:59 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
24/06/12 17:18:59 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
