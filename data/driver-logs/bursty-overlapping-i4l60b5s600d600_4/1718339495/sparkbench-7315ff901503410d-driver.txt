Files  local:///opt/sparkbench/sparkbench_2.12-1.0.jar from /opt/sparkbench/sparkbench_2.12-1.0.jar to /opt/spark/work-dir/./sparkbench_2.12-1.0.jar
24/06/14 04:31:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/14 04:31:02 WARN SparkContext: The JAR local:///opt/sparkbench/sparkbench_2.12-1.0.jar at file:/opt/sparkbench/sparkbench_2.12-1.0.jar has been added already. Overwriting of added jar is not supported in the current version.
24/06/14 04:31:04 INFO ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 8, known: 0, sharedSlotFromPendingPods: 2147483647.
24/06/14 04:31:04 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/06/14 04:31:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
24/06/14 04:31:05 INFO ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 8, known: 3, sharedSlotFromPendingPods: 2147483644.
24/06/14 04:31:05 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/06/14 04:31:06 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-events/spark-09d6a799f3ad4532a9fbb17014bae8e0.inprogress. This is unsupported
24/06/14 04:31:06 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 8, known: 6, sharedSlotFromPendingPods: 2147483642.
24/06/14 04:31:06 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
24/06/14 04:31:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.8.210:60460
24/06/14 04:31:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.10.225:36246
24/06/14 04:31:12 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.6.213:59024
24/06/14 04:31:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.10.225:58648) with ID 2,  ResourceProfileId 0
24/06/14 04:31:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.8.210:60462) with ID 3,  ResourceProfileId 0
24/06/14 04:31:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.6.213:59034) with ID 1,  ResourceProfileId 0
24/06/14 04:31:13 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.12.222:58316
24/06/14 04:31:14 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.11.221:42912
24/06/14 04:31:14 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.9.211:52494
24/06/14 04:31:14 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.12.222:58322) with ID 4,  ResourceProfileId 0
24/06/14 04:31:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.11.221:42920) with ID 6,  ResourceProfileId 0
24/06/14 04:31:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.7.213:55566
24/06/14 04:31:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.9.211:52498) with ID 5,  ResourceProfileId 0
24/06/14 04:31:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.1.223:47808
24/06/14 04:31:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.7.213:55582) with ID 8,  ResourceProfileId 0
24/06/14 04:31:16 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
24/06/14 04:31:16 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.1.223:47818) with ID 7,  ResourceProfileId 0
Custom start time given: 1718339495
Submission for query 'q12' at T+0 ms waiting for its time to shine...
Submitting 'q12' at T+0 ms (43 ms delay)...
Submission for query 'q7' at T+0 ms waiting for its time to shine...
Submitting 'q7' at T+0 ms (50 ms delay)...
Submission for query 'q13' at T+0 ms waiting for its time to shine...
Submitting 'q13' at T+0 ms (51 ms delay)...
Submission for query 'q13' at T+0 ms waiting for its time to shine...
Submitting 'q13' at T+0 ms (52 ms delay)...
Submission for query 'q13' at T+180000 ms waiting for its time to shine...
Query 'q12' at T+0 ms submitted (0 ms after submission was started).
Query 'q13' at T+0 ms submitted (0 ms after submission was started).
Query 'q13' at T+0 ms submitted (0 ms after submission was started).
Query 'q7' at T+0 ms submitted (0 ms after submission was started).
24/06/14 04:33:06 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
24/06/14 04:33:06 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
Submission for query 'q7' at T+0 ms finished in 127488 ms!
Submission for query 'q12' at T+0 ms finished in 132222 ms!
Submission for query 'q13' at T+0 ms finished in 136224 ms!
Submission for query 'q13' at T+0 ms finished in 139306 ms!
Submitting 'q13' at T+180000 ms (93 ms delay)...
Submission for query 'q13' at T+180000 ms waiting for its time to shine...
Submitting 'q13' at T+180000 ms (94 ms delay)...
Query 'q13' at T+180000 ms submitted (0 ms after submission was started).
Submission for query 'q12' at T+180000 ms waiting for its time to shine...
Submitting 'q12' at T+180000 ms (95 ms delay)...
Query 'q13' at T+180000 ms submitted (0 ms after submission was started).
Submission for query 'q7' at T+180000 ms waiting for its time to shine...
Submitting 'q7' at T+180000 ms (96 ms delay)...
Query 'q12' at T+180000 ms submitted (0 ms after submission was started).
Submission for query 'q7' at T+420000 ms waiting for its time to shine...
Query 'q7' at T+180000 ms submitted (0 ms after submission was started).
24/06/14 04:34:39 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
24/06/14 04:35:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
Submission for query 'q7' at T+180000 ms finished in 71679 ms!
Submission for query 'q13' at T+180000 ms finished in 71757 ms!
Submission for query 'q12' at T+180000 ms finished in 72597 ms!
Submission for query 'q13' at T+180000 ms finished in 77085 ms!
Submitting 'q7' at T+420000 ms (44 ms delay)...
Submission for query 'q13' at T+420000 ms waiting for its time to shine...
Submitting 'q13' at T+420000 ms (46 ms delay)...
Query 'q7' at T+420000 ms submitted (0 ms after submission was started).
Submission for query 'q12' at T+420000 ms waiting for its time to shine...
Submitting 'q12' at T+420000 ms (49 ms delay)...
Query 'q13' at T+420000 ms submitted (0 ms after submission was started).
Submission for query 'q10' at T+420000 ms waiting for its time to shine...
Submitting 'q10' at T+420000 ms (51 ms delay)...
Query 'q12' at T+420000 ms submitted (0 ms after submission was started).
Submission for query 'q7' at T+660000 ms waiting for its time to shine...
Query 'q10' at T+420000 ms submitted (0 ms after submission was started).
24/06/14 04:38:49 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
Submission for query 'q12' at T+420000 ms finished in 55548 ms!
Submission for query 'q7' at T+420000 ms finished in 55707 ms!
24/06/14 04:39:33 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB
Submission for query 'q13' at T+420000 ms finished in 60323 ms!
24/06/14 04:39:37 WARN DAGScheduler: Broadcasting large task binary with size 1136.4 KiB
24/06/14 04:39:40 WARN DAGScheduler: Broadcasting large task binary with size 1130.4 KiB
Submission for query 'q10' at T+420000 ms finished in 65943 ms!
Submitting 'q7' at T+660000 ms (29 ms delay)...
Submission for query 'q13' at T+660000 ms waiting for its time to shine...
Submitting 'q13' at T+660000 ms (32 ms delay)...
Query 'q7' at T+660000 ms submitted (0 ms after submission was started).
Submission for query 'q10' at T+660000 ms waiting for its time to shine...
Submitting 'q10' at T+660000 ms (32 ms delay)...
Submission for query 'q10' at T+660000 ms waiting for its time to shine...
Submitting 'q10' at T+660000 ms (33 ms delay)...
Query 'q13' at T+660000 ms submitted (0 ms after submission was started).
Query 'q10' at T+660000 ms submitted (0 ms after submission was started).
Query 'q10' at T+660000 ms submitted (0 ms after submission was started).
24/06/14 04:43:00 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB
24/06/14 04:43:20 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB
24/06/14 04:43:20 WARN DAGScheduler: Broadcasting large task binary with size 1046.5 KiB
Submission for query 'q7' at T+660000 ms finished in 57577 ms!
24/06/14 04:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1152.6 KiB
24/06/14 04:43:40 WARN DAGScheduler: Broadcasting large task binary with size 1154.5 KiB
Submission for query 'q13' at T+660000 ms finished in 70652 ms!
24/06/14 04:43:45 WARN DAGScheduler: Broadcasting large task binary with size 1134.0 KiB
Submission for query 'q10' at T+660000 ms finished in 71043 ms!
24/06/14 04:43:48 WARN DAGScheduler: Broadcasting large task binary with size 1136.0 KiB
Submission for query 'q10' at T+660000 ms finished in 73467 ms!
thread pool status is: true
run finished
main returning...
24/06/14 04:43:48 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
24/06/14 04:43:48 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
24/06/14 04:43:48 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.
